{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font color=navy>Big Data Economics</font></center>\n",
    "### <center>Case Study: Recommender Systems</center>\n",
    "#### <center>Ali Habibnia</center>\n",
    "\n",
    "    \n",
    "<center> Assistant Professor, Department of Economics, </center>\n",
    "<center> and Division of Computational Modeling & Data Analytics at Virginia Tech</center>\n",
    "<center> habibnia@vt.edu </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MPc8aWJR0h4d"
   },
   "source": [
    "<img src=\"images/recom.png\" alt=\"Drawing\" style=\"width: 650px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "As a showcase we are going to implement a recommender system. These days we are constantly being recommended from varieties of sources to what content to read (Facebook), what music to listen to (Spotify), etc…\n",
    "\n",
    "**Recommender system strategies**\n",
    "\n",
    "**Content-based Filtering** → creates a profile for each user or product to characterize its nature\n",
    "\n",
    "**Collaborative Filtering** → analyzes relationships between users and inter-dependencies among products to identify new user-item associations\n",
    "\n",
    "Collaborative filtering is generally more accurate then content filtering however, it suffers from cold start problem. (If new user exist and does not have any inter-dependencies among others, we can’t recommend anything). In general there is two method to achieve Collaborative filtering: Neighborhood Methods and Latent Factor (Matrix factorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qq0oNGb80h7-"
   },
   "source": [
    "### Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mT_aJLIu0h_W"
   },
   "source": [
    "Here we are going to investigate Matrix Factorization. When a user gives feed back to a certain movie they saw (say they can rate from one to five), this collection of feedback can be represented in a form of a matrix. Where each row represents each users, while each column represents different movies. Obviously the matrix will be sparse since not everyone is going to watch every movies, (we all have different taste when it comes to movies).\n",
    "\n",
    "One strength of matrix factorization is the fact that it can incorporate implicit feedback, information that are not directly given but can be derived by analyzing user behavior. Using this strength we can estimate if a user is going to like a movie that he/she never saw. And if that estimated rating is high, we can recommend that movie to the user.\n",
    "\n",
    "<img src=\"images/rating_matrix.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "The above image does an excellent job of summarizing, the core idea behind matrix factorization. Let there be matrix $A$ with dimensionality of $(m,n)$ this matrix can be viewed as a dot product between two matrix with each matrices having dimensions of $(m,k)$ and $(k,n)$.\n",
    "\n",
    "> Just as a side note, this Matrix Factorization is heavily related to Singular Value Decomposition (SVD). One downside of SVD is the fact that when the original matrix is sparse (incomplete) left and right singular vectors are undefined. So there are ways to impute missing values. Instead of using SVD we are going to use an iterative method called FunkSVD which became popular during the famous Netflix challenge. Read this interesting [New York Times article](https://www.nytimes.com/2008/11/23/magazine/23Netflix-t.html?pagewanted=all&_r=0) for more information.\n",
    "\n",
    "$k$ is the number of latent factors. The algorithm finds $k$ **latent** characteristic for each movie and user. That is they are coding some meaningful aspect of the movies and user profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kZx1RiaaIGde"
   },
   "source": [
    "Movielens is a dataset of (user, movie, rating) triples. Given training data of such triples, we want to predict ratings for unseen (user, movie) pairs. All ratings are between 0.5 and 5.0. \n",
    "\n",
    "First, let's download and extract the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3241,
     "status": "ok",
     "timestamp": 1547849323771,
     "user": {
      "displayName": "Ali Habibnia",
      "photoUrl": "https://lh5.googleusercontent.com/-5GKSE96mbNQ/AAAAAAAAAAI/AAAAAAAAAAc/CeIIA7yQ6Rg/s64/photo.jpg",
      "userId": "02902082094624308811"
     },
     "user_tz": 300
    },
    "id": "RemNbOoz0iCk",
    "outputId": "b51d2fbc-88e2-4480-da8a-53cdfeb53aae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  955k  100  955k    0     0  1647k      0 --:--:-- --:--:-- --:--:-- 1647k0     0   253k      0  0:00:03 --:--:--  0:00:03  252k\n",
      "Archive:  ml-latest-small.zip\n",
      "  inflating: ml-latest-small/links.csv  \n",
      "  inflating: ml-latest-small/tags.csv  \n",
      "  inflating: ml-latest-small/ratings.csv  \n",
      "  inflating: ml-latest-small/README.txt  \n",
      "  inflating: ml-latest-small/movies.csv  \n"
     ]
    }
   ],
   "source": [
    "! curl http://files.grouplens.org/datasets/movielens/ml-latest-small.zip -o ml-latest-small.zip\n",
    "! unzip -o ml-latest-small.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BYzN2egg0iF2"
   },
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "import pandas as pd\n",
    "movies_df = pd.read_csv('ml-latest-small/movies.csv')\n",
    "ratings_df = pd.read_csv('ml-latest-small/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1547849339201,
     "user": {
      "displayName": "Ali Habibnia",
      "photoUrl": "https://lh5.googleusercontent.com/-5GKSE96mbNQ/AAAAAAAAAAI/AAAAAAAAAAc/CeIIA7yQ6Rg/s64/photo.jpg",
      "userId": "02902082094624308811"
     },
     "user_tz": 300
    },
    "id": "n-0WySEgIYib",
    "outputId": "a0618a50-41ee-42c3-dae5-8f8d00de67e1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1547849345382,
     "user": {
      "displayName": "Ali Habibnia",
      "photoUrl": "https://lh5.googleusercontent.com/-5GKSE96mbNQ/AAAAAAAAAAI/AAAAAAAAAAc/CeIIA7yQ6Rg/s64/photo.jpg",
      "userId": "02902082094624308811"
     },
     "user_tz": 300
    },
    "id": "J-UE3kgHJZMb",
    "outputId": "faa41525-c41b-4911-a09a-a0f1b1afdceb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PijaU5KDMRdF"
   },
   "source": [
    "Let's first install torch library. This may take a minute or two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/torch/distro.git ~/torch --recursive\n",
    "!cd ~/torch; bash install-deps;\n",
    "!./install.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLMe0GlfhyO2"
   },
   "source": [
    "Now let's define the model in PyTorch. You can see that the affinity of the user for an item (movie) is calculated by multiplying the user latent vector by the movie latent vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F71KHB89IYmz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "class MatrixFactorization(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=20):\n",
    "      super().__init__()\n",
    "      # create user embeddings\n",
    "      self.user_factors = torch.nn.Embedding(n_users, n_factors)\n",
    "      # create item embeddings\n",
    "      self.item_factors = torch.nn.Embedding(n_items, n_factors)\n",
    "      self.user_factors.weight.data.uniform_(0, 0.05)\n",
    "      self.item_factors.weight.data.uniform_(0, 0.05)\n",
    "\n",
    "    def forward(self, data):\n",
    "      # matrix multiplication\n",
    "      users, items = data[:,0], data[:,1]\n",
    "      return (self.user_factors(users)*self.item_factors(items)).sum(1)\n",
    "\n",
    "    def predict(self, user, item):\n",
    "        return self.forward(user, item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "leYTWO0Oh5hR"
   },
   "source": [
    "Then we implement a loader for our dataset. You will later see why feeding (mini-) batches of data can improve the computational performance of the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 461,
     "status": "ok",
     "timestamp": 1547849571484,
     "user": {
      "displayName": "Ali Habibnia",
      "photoUrl": "https://lh5.googleusercontent.com/-5GKSE96mbNQ/AAAAAAAAAAI/AAAAAAAAAAc/CeIIA7yQ6Rg/s64/photo.jpg",
      "userId": "02902082094624308811"
     },
     "user_tz": 300
    },
    "id": "k3mfVPvYh4Mw",
    "outputId": "7b377185-281a-4f62-ee99-023aeac43028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 610\n",
      "Number of movies: 9724\n",
      "Number of ratings: 100836\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataloader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Loader(Dataset):\n",
    "  def __init__(self):\n",
    "    self.ratings = ratings_df.copy()\n",
    "    # Extract all user IDs and movie IDs\n",
    "    users = ratings_df.userId.unique()\n",
    "    movies = ratings_df.movieId.unique()\n",
    "\n",
    "    #Producing new continuous IDs for users and movies\n",
    "    self.userid2idx = {o:i for i,o in enumerate(users)}\n",
    "    self.movieid2idx = {o:i for i,o in enumerate(movies)}\n",
    "    \n",
    "    self.idx2userid = {i:o for o,i in self.userid2idx.items()}\n",
    "    self.idx2movieid = {i:o for o,i in self.movieid2idx.items()}\n",
    "\n",
    "    self.ratings.movieId = ratings_df.movieId.apply(lambda x: self.movieid2idx[x])\n",
    "    self.ratings.userId = ratings_df.userId.apply(lambda x: self.userid2idx[x])\n",
    "\n",
    "    self.x = self.ratings.drop(['rating', 'timestamp'], axis=1).values\n",
    "    self.y = self.ratings['rating'].values\n",
    "    self.x, self.y  = torch.tensor(self.x), torch.tensor(self.y)\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    return (self.x[index], self.y[index])\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.ratings)\n",
    "\n",
    "  \n",
    "# Movie ID to movie name mapping\n",
    "movie_names = movies_df.set_index('movieId')['title'].to_dict()\n",
    "\n",
    "\n",
    "n_users = len(ratings_df.userId.unique())\n",
    "n_items = len(ratings_df.movieId.unique())\n",
    "\n",
    "print(\"Number of users:\", n_users)\n",
    "print(\"Number of movies:\", n_items)\n",
    "print(\"Number of ratings:\", len(ratings_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fcqiMIbdKI0h"
   },
   "source": [
    "We have 610 users and 9724 movies. That is the full rating matrix has 5931640 elements. But we have only 100836 ratings. So only 1.69% of the matrix is filled and it is very sparse. If the matrix is stored with `float32` data type, it would only need ~22MB to be stored in RAM. But movielens also publishes much larger rating datasets in orders of millions. As the number of users and movies grow, the size of the rating matrix grows exponentially, to a extent that storing the full matrix in memory would be a challenge.\n",
    "\n",
    "An advantage of the current method to SVD is that the rating matrix is realized implicitly and we don't need to form the full dense matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oMdiXCFyLg5f"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-7QQWrLVXGX"
   },
   "source": [
    "Next we want to train (fit) the model. So we had two matrices with user and movie latent factors (embeddings). That is one vector for each movie and one vector for each user. \n",
    "\n",
    "The matrices holding latent factors are initially filled with random variables. That is, they would not make sensible prediction if we multiply user and movie vectors to get ratings. \n",
    "\n",
    "Next we are going to use SGD (Stochastic Gradient Descent) to find the proper values for the latent factors. \n",
    "\n",
    "SGD works by changing value of variables such that value of some function is minimized. We call this function the **loss function**.  Here the loss function is how much the prediction of the rating for a known rating deviates from the actual value. By minimizing this **error**, we hope to find parameter values that will **generalize** to unseen user-movie pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 726
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36477,
     "status": "ok",
     "timestamp": 1547849613446,
     "user": {
      "displayName": "Ali Habibnia",
      "photoUrl": "https://lh5.googleusercontent.com/-5GKSE96mbNQ/AAAAAAAAAAI/AAAAAAAAAAc/CeIIA7yQ6Rg/s64/photo.jpg",
      "userId": "02902082094624308811"
     },
     "user_tz": 300
    },
    "id": "wFUXcsndWpr0",
    "outputId": "44258714-93a0-44df-f1a0-608c0cdf7268"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is running on GPU: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm_notebook.py:88: TqdmExperimentalWarning: Detect Google Colab 0.0.1a2 and thus load dummy ipywidgets package. Note that UI is different from that in Jupyter. See https://github.com/tqdm/tqdm/pull/640\n",
      "  \" See https://github.com/tqdm/tqdm/pull/640\".format(colab.__version__), TqdmExperimentalWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"display:flex;flex-direction:row;\"><span></span><progress style='margin:2px 4px;' max='15' value='15'></progress>100% 15/15 [00:36&lt;00:00,  2.43s/it]</div>"
      ],
      "text/plain": [
       "<tqdm._fake_ipywidgets.HBox object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter #0 Loss: 11.070071899951412\n",
      "iter #1 Loss: 4.7471233691055765\n",
      "iter #2 Loss: 2.477282887939269\n",
      "iter #3 Loss: 1.7224268771065068\n",
      "iter #4 Loss: 1.346314886197221\n",
      "iter #5 Loss: 1.128931223287195\n",
      "iter #6 Loss: 0.9916708728837483\n",
      "iter #7 Loss: 0.900546970403739\n",
      "iter #8 Loss: 0.8374471417462765\n",
      "iter #9 Loss: 0.7924228728876501\n",
      "iter #10 Loss: 0.7594797659646436\n",
      "iter #11 Loss: 0.7348613678879544\n",
      "iter #12 Loss: 0.7162211461720733\n",
      "iter #13 Loss: 0.7017044715469863\n",
      "iter #14 Loss: 0.6905001870871801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "cuda = torch.cuda.is_available()\n",
    "print(\"Is running on GPU:\", cuda)\n",
    "model = MatrixFactorization(n_users, n_items, n_factors=8)\n",
    "if cuda:\n",
    "  model = model.cuda()\n",
    "loss_fn = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr=1e-3)\n",
    "\n",
    "train_set = Loader()\n",
    "train_loader = DataLoader(train_set, 128, shuffle=True)\n",
    "\n",
    "for it in tqdm(range(num_epochs)):\n",
    "  losses = []\n",
    "  for x, y in train_loader:\n",
    "    if cuda:\n",
    "      x, y = x.cuda(), y.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x)\n",
    "\n",
    "    loss = loss_fn(outputs.squeeze(), y.type(torch.float32))\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  print(\"iter #{}\".format(it), \"Loss:\", sum(losses) / len(losses))\n",
    "  \n",
    "#   break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_jw7_QUaLuPJ"
   },
   "source": [
    "By training the model, we will have the tuned latent factors for movies and users. \n",
    "\n",
    "Now what if we want to propose some movies to a user to watch. We simple take the user's latent factors and multiply them by every movie latent factors. As a result, we will have the predicted rating of the user for each movie. Then we sort those movies, in descending order or predicted rating, and report back the top items to the user as the movies that he would probably enjoy watching. Voila!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ISfATdNPM2VW"
   },
   "source": [
    "#### Clustering Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ma6NedfOM5w9"
   },
   "source": [
    "The ML task we investigated was an example of supervised learning. We were told the ratings of some users for some movies and we are expected to be able to predict how a user would predict an unseen movie.\n",
    "\n",
    "Now, we are going to see an example of unsupervised learning. As we mentioned, the latent factors represent some aspects of the movies and users. So for example latent factor #1 might be an indicator of the movie being a classic movie or not. What the algorithm finds is usually more complicated than what can be put into words, but if you plot the movies based on latent factors, you may see that clearly the can distinguish movies based on some criteria that the algorithm itself finds. A side product of having latent factors is that we can find similar movies by unsupervised learning. The movies that are similar tend to have latent factors that are \"closer\" to each other. Based on this observation we use a famous clustering algorithm called kMeans to put movies into a set of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9zFHL0pcWp2U"
   },
   "outputs": [],
   "source": [
    "trained_movie_embeddings = model.item_factors.weight.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BhGdzOO1Wp6w"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(trained_movie_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tw1fL8L5OvLX"
   },
   "source": [
    "Next we are going to show 10 blockbuster movies in each category. It can be seen that movies that are put into the same cluster tend to have similar genre. Notice that the algorithm was never aware of the movies' name or genre and it somehow extracted this information merely by looking at a bunch of numbers representing how some users responded to some movies.\n",
    "\n",
    "Similar to this, a web service provider may do this on users' latent factor to suggest friendship among users with similar taste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1887
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11825,
     "status": "ok",
     "timestamp": 1547849790118,
     "user": {
      "displayName": "Ali Habibnia",
      "photoUrl": "https://lh5.googleusercontent.com/-5GKSE96mbNQ/AAAAAAAAAAI/AAAAAAAAAAc/CeIIA7yQ6Rg/s64/photo.jpg",
      "userId": "02902082094624308811"
     },
     "user_tz": 300
    },
    "id": "QCDCux6bWERJ",
    "outputId": "a08f0170-ae42-4c19-9e7b-674f319e3ae5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #0\n",
      "\t Jurassic Park (1993)\n",
      "\t Terminator 2: Judgment Day (1991)\n",
      "\t Toy Story (1995)\n",
      "\t Seven (a.k.a. Se7en) (1995)\n",
      "\t Apollo 13 (1995)\n",
      "\t Fugitive, The (1993)\n",
      "\t Lord of the Rings: The Two Towers, The (2002)\n",
      "\t Aladdin (1992)\n",
      "\t Sixth Sense, The (1999)\n",
      "\t Twelve Monkeys (a.k.a. 12 Monkeys) (1995)\n",
      "Cluster #1\n",
      "\t Batman Forever (1995)\n",
      "\t Waterworld (1995)\n",
      "\t Ace Ventura: When Nature Calls (1995)\n",
      "\t Nutty Professor, The (1996)\n",
      "\t Mission: Impossible II (2000)\n",
      "\t Charlie's Angels (2000)\n",
      "\t Honey, I Shrunk the Kids (1989)\n",
      "\t Lost World: Jurassic Park, The (1997)\n",
      "\t Austin Powers in Goldmember (2002)\n",
      "\t Blair Witch Project, The (1999)\n",
      "Cluster #2\n",
      "\t Speed 2: Cruise Control (1997)\n",
      "\t Battlefield Earth (2000)\n",
      "\t Problem Child (1990)\n",
      "\t Flintstones in Viva Rock Vegas, The (2000)\n",
      "\t Spice World (1997)\n",
      "\t Stuart Saves His Family (1995)\n",
      "\t Catwoman (2004)\n",
      "\t Problem Child 2 (1991)\n",
      "\t Texas Chainsaw Massacre, The (2003)\n",
      "\t Rollerball (2002)\n",
      "Cluster #3\n",
      "\t Independence Day (a.k.a. ID4) (1996)\n",
      "\t Batman (1989)\n",
      "\t True Lies (1994)\n",
      "\t Mrs. Doubtfire (1993)\n",
      "\t Stargate (1994)\n",
      "\t Pretty Woman (1990)\n",
      "\t GoldenEye (1995)\n",
      "\t Twister (1996)\n",
      "\t Interview with the Vampire: The Vampire Chronicles (1994)\n",
      "\t Star Trek: Generations (1994)\n",
      "Cluster #4\n",
      "\t Wild Wild West (1999)\n",
      "\t Godzilla (1998)\n",
      "\t I Know What You Did Last Summer (1997)\n",
      "\t Anaconda (1997)\n",
      "\t Richie Rich (1994)\n",
      "\t Super Mario Bros. (1993)\n",
      "\t Inspector Gadget (1999)\n",
      "\t Honey, I Blew Up the Kid (1992)\n",
      "\t I Still Know What You Did Last Summer (1998)\n",
      "\t Rocky V (1990)\n",
      "Cluster #5\n",
      "\t Forrest Gump (1994)\n",
      "\t Shawshank Redemption, The (1994)\n",
      "\t Pulp Fiction (1994)\n",
      "\t Silence of the Lambs, The (1991)\n",
      "\t Matrix, The (1999)\n",
      "\t Star Wars: Episode IV - A New Hope (1977)\n",
      "\t Braveheart (1995)\n",
      "\t Schindler's List (1993)\n",
      "\t Fight Club (1999)\n",
      "\t Star Wars: Episode V - The Empire Strikes Back (1980)\n",
      "Cluster #6\n",
      "\t Speed (1994)\n",
      "\t Men in Black (a.k.a. MIB) (1997)\n",
      "\t Mission: Impossible (1996)\n",
      "\t Die Hard: With a Vengeance (1995)\n",
      "\t Titanic (1997)\n",
      "\t Babe (1995)\n",
      "\t E.T. the Extra-Terrestrial (1982)\n",
      "\t Spider-Man (2002)\n",
      "\t Rock, The (1996)\n",
      "\t Minority Report (2002)\n",
      "Cluster #7\n",
      "\t Jason X (2002)\n",
      "\t Police Academy: Mission to Moscow (1994)\n",
      "\t House Party 2 (1991)\n",
      "\t Epic Movie (2007)\n",
      "\t Wicker Man, The (2006)\n",
      "\t House of the Dead, The (2003)\n",
      "\t Fifty Shades of Grey (2015)\n",
      "\t Yogi Bear (2010)\n",
      "\t From Justin to Kelly (2003)\n",
      "\t Disaster Movie (2008)\n",
      "Cluster #8\n",
      "\t Ace Ventura: Pet Detective (1994)\n",
      "\t Mask, The (1994)\n",
      "\t Star Wars: Episode I - The Phantom Menace (1999)\n",
      "\t Dumb & Dumber (Dumb and Dumber) (1994)\n",
      "\t Austin Powers: The Spy Who Shagged Me (1999)\n",
      "\t Home Alone (1990)\n",
      "\t Net, The (1995)\n",
      "\t Cliffhanger (1993)\n",
      "\t Natural Born Killers (1994)\n",
      "\t Armageddon (1998)\n",
      "Cluster #9\n",
      "\t Coneheads (1993)\n",
      "\t Judge Dredd (1995)\n",
      "\t Mortal Kombat (1995)\n",
      "\t Species (1995)\n",
      "\t Flintstones, The (1994)\n",
      "\t Batman & Robin (1997)\n",
      "\t Striptease (1996)\n",
      "\t Hollow Man (2000)\n",
      "\t Free Willy (1993)\n",
      "\t Showgirls (1995)\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(10):\n",
    "  print(\"Cluster #{}\".format(cluster))\n",
    "  movs = []\n",
    "  for movidx in np.where(kmeans.labels_ == cluster)[0]:\n",
    "    movid = train_set.idx2movieid[movidx]\n",
    "    rat_count = ratings_df.loc[ratings_df['movieId']==movid].count()[0]\n",
    "    movs.append((movie_names[movid], rat_count))\n",
    "  for mov in sorted(movs, key=lambda tup: tup[1], reverse=True)[:10]:\n",
    "    print(\"\\t\", mov[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
