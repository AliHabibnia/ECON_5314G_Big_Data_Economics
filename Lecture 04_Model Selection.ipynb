{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font color=navy>Big Data Economics</font></center>\n",
    "### <center>Model Selection</center>\n",
    "#### <center>Ali Habibnia</center>\n",
    "\n",
    "<center> Assistant Professor, Department of Economics, </center>\n",
    "<center> and Division of Computational Modeling & Data Analytics at Virginia Tech</center>\n",
    "<center> habibnia@vt.edu </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "### <center>Frequentism vs Bayesianism</center>\n",
    "\n",
    "One of the first things a scientist hears about statistics is that there are two different approaches: frequentism and Bayesianism. Fundamentally, the disagreement between frequentists and Bayesians concerns the definition of probability.\n",
    "\n",
    "<img src=\"images/bayvsfre.jpg\"  width=\"600\">\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readings:\n",
    "\n",
    "1. ***Chapter 6,*** Graham Elliott, and Allan Timmermann, Economic Forecasting, Princeton University Press, 2016.\n",
    "2. ***Chapter 7.5 - 7.6 - 7.7,*** [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals\n",
    "\n",
    "- When we have many variables/predictors (with many possible interactions), it can be difficult to find a good model.\n",
    "- Which main effects do we include?\n",
    "- Which interactions do we include?\n",
    "\n",
    "> Model selection tries to answer to this questions.\n",
    "> we need:\n",
    ">- a criterion or benchmark to compare two models.\n",
    ">- a search strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting vs Model Selection\n",
    "\n",
    "The difference between *model fitting* and *model selection* is often a cause of confusion.\n",
    "**Model fitting** proceeds by assuming a particular model is true, and tuning the model so it provides the best possible fit to the data. **Model selection** (which includes *model comparison*, and *model evaluation*) , on the other hand, asks the larger question of whether the assumptions of the model are compatible with the data.\n",
    "\n",
    "Let's make this more concrete.\n",
    "By *model* here I essentially mean a formula, usually with tunable parameters, which quantifies the likelihood of observing your data.\n",
    "For example, your model might consist of the statement, \"the $(x, y)$ observations come from a straight line, with known normal measurement errors $\\sigma_y$\".\n",
    "Labeling this model $M_1$, we can write:\n",
    "\n",
    "$$\n",
    "y_{M_1}(x;\\theta) = \\theta_0 + \\theta_1 x\\\\\n",
    "y \\sim \\mathcal{N}(y_{M_1}, \\sigma_y^2)\n",
    "$$\n",
    "\n",
    "where the second line indicates that the observed $y$ is normally distributed about the model value, with variance $\\sigma_y^2$.\n",
    "There are two tunable parameters to this model, represented by the vector $\\theta = [\\theta_0, \\theta_1]$ (i.e. the slope and intercept).\n",
    "\n",
    "Another model might consist of the statement \"the observations $(x, y)$ come from a quadratic curve, with known normal measurement errors $\\sigma_y$\".\n",
    "Labeling this model $M_2$, we can write:\n",
    "\n",
    "$$\n",
    "y_{M_2}(x;\\theta) = \\theta_0 + \\theta_1 x + \\theta_2 x^2\\\\\n",
    "y \\sim \\mathcal{N}(y_{M_2}, \\sigma_y^2)\n",
    "$$\n",
    "\n",
    "There are three tunable parameters here, again represented by the vector $\\theta$.\n",
    "\n",
    "Model fitting, in this case, is the process of finding constraints on the values of the parameters $\\theta$ within each model.\n",
    "That is, it allows you to make statements such as, \"assuming $M_1$ is true, this particular $\\theta$ gives the best-fit line\" or \"assuming $M_2$ is true, this particular vector $\\theta$ gives the best-fit curve.\"\n",
    "Model fitting proceeds without respect to whether the model is capable of describing the data well; it just arrives at the best-fit model *under the assumption that the model is accurate*.\n",
    "\n",
    "> Model selection, on the other hand, is not concerned with the parameters themselves, but with the question of whether the model is capable of describing the data well.\n",
    "That is, it allows you to say, \"for my data, a line ($M_1$) provides a better fit than a quadratic curve ($M_2$)\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In particular, we wish to select the model that performs optimally, both with respect to the training data (in-sample data) and to external data (out-of-sample).\n",
    "\n",
    "\n",
    "- How many variables should be included in the model? Here's one idea:\n",
    "  <br>  \n",
    "    - Try different models, and only keep predictors in the model if they have small p-values. (Stepwise regression, our next session topic!)\n",
    "    - Check whether the adjusted R-squared value goes up when you add new predictors.\n",
    "\n",
    "- What functional form should be used in fitting the model?\n",
    "- What sort of polynomial relationship best describes the relationship?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Advertising Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this allows plots to appear directly in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  radio  newspaper  sales\n",
       "1  230.1   37.8       69.2   22.1\n",
       "2   44.5   39.3       45.1   10.4\n",
       "3   17.2   45.9       69.3    9.3\n",
       "4  151.5   41.3       58.5   18.5\n",
       "5  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data into a DataFrame\n",
    "data = pd.read_csv('https://www.statlearning.com/s/Advertising.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the shape of the DataFrame\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept    7.032594\n",
       "TV           0.047537\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the standard import if you're using \"formula notation\" (similar to R)\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# create a fitted model in one line\n",
    "lm = smf.ols(formula='sales ~ TV', data=data).fit()\n",
    "\n",
    "# print the coefficients\n",
    "lm.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>6.129719</td>\n",
       "      <td>7.935468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TV</th>\n",
       "      <td>0.042231</td>\n",
       "      <td>0.052843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1\n",
       "Intercept  6.129719  7.935468\n",
       "TV         0.042231  0.052843"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the confidence intervals for the model coefficients\n",
    "lm.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept    1.406300e-35\n",
       "TV           1.467390e-42\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the p-values for the model coefficients\n",
    "lm.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.611875050850071"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the R-squared value for the model\n",
    "lm.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept    2.938889\n",
       "TV           0.045765\n",
       "radio        0.188530\n",
       "newspaper   -0.001037\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a fitted model with all three features\n",
    "lm = smf.ols(formula='sales ~ TV + radio + newspaper', data=data).fit()\n",
    "\n",
    "# print the coefficients\n",
    "lm.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8972106381789522"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the R-squared value for the model\n",
    "lm.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept    1.267295e-17\n",
       "TV           1.509960e-81\n",
       "radio        1.505339e-54\n",
       "newspaper    8.599151e-01\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the p-values for the model coefficients\n",
    "lm.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>sales</td>      <th>  R-squared:         </th> <td>   0.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.896</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   570.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 13 Feb 2020</td> <th>  Prob (F-statistic):</th> <td>1.58e-96</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:27:55</td>     <th>  Log-Likelihood:    </th> <td> -386.18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   780.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   196</td>      <th>  BIC:               </th> <td>   793.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    2.9389</td> <td>    0.312</td> <td>    9.422</td> <td> 0.000</td> <td>    2.324</td> <td>    3.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TV</th>        <td>    0.0458</td> <td>    0.001</td> <td>   32.809</td> <td> 0.000</td> <td>    0.043</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>radio</th>     <td>    0.1885</td> <td>    0.009</td> <td>   21.893</td> <td> 0.000</td> <td>    0.172</td> <td>    0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>newspaper</th> <td>   -0.0010</td> <td>    0.006</td> <td>   -0.177</td> <td> 0.860</td> <td>   -0.013</td> <td>    0.011</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>60.414</td> <th>  Durbin-Watson:     </th> <td>   2.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 151.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.327</td> <th>  Prob(JB):          </th> <td>1.44e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.332</td> <th>  Cond. No.          </th> <td>    454.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  sales   R-squared:                       0.897\n",
       "Model:                            OLS   Adj. R-squared:                  0.896\n",
       "Method:                 Least Squares   F-statistic:                     570.3\n",
       "Date:                Thu, 13 Feb 2020   Prob (F-statistic):           1.58e-96\n",
       "Time:                        17:27:55   Log-Likelihood:                -386.18\n",
       "No. Observations:                 200   AIC:                             780.4\n",
       "Df Residuals:                     196   BIC:                             793.6\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      2.9389      0.312      9.422      0.000       2.324       3.554\n",
       "TV             0.0458      0.001     32.809      0.000       0.043       0.049\n",
       "radio          0.1885      0.009     21.893      0.000       0.172       0.206\n",
       "newspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n",
       "==============================================================================\n",
       "Omnibus:                       60.414   Durbin-Watson:                   2.084\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\n",
       "Skew:                          -1.327   Prob(JB):                     1.44e-33\n",
       "Kurtosis:                       6.332   Cond. No.                         454.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a summary of the fitted model\n",
    "lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8961505479974429"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only include TV and Radio in the model\n",
    "lm = smf.ols(formula='sales ~ TV + radio', data=data).fit()\n",
    "lm.rsquared_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8956373316204668"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add Newspaper to the model (which we believe has no association with Sales)\n",
    "lm = smf.ols(formula='sales ~ TV + radio + newspaper', data=data).fit()\n",
    "lm.rsquared_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048.687962269444"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only include TV and Radio in the model\n",
    "lm = smf.ols(formula='sales ~ TV', data=data).fit()\n",
    "lm.bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "788.2890508023243"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only include TV and Radio in the model\n",
    "lm = smf.ols(formula='sales ~ TV + radio', data=data).fit()\n",
    "lm.bic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Approach: Comparing Maximum Likelihoods\n",
    "\n",
    "One common mistake is to assume that we can select between models via *the value of the maximum likelihood*.\n",
    "While this works in some special cases, it is not generally applicable. The model with more variables yields a higher log-likelihood, but this **does not** necessarily mean it is the better model!\n",
    "\n",
    "The problem is that when a model has more degrees of freedom, will **always** give an equal or larger maximum likelihood, regardless of the data! This trend holds generally: as you increase model complexity, the maximum likelihood value will (almost) always increase!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing R2 and Adjusted R2\n",
    "\n",
    "**R-squared will always increase as you add more features to the model**, even if they are unrelated to the response. Thus, selecting the model with the highest R-squared is not a reliable approach for choosing the best linear model.\n",
    "\n",
    "There is alternative to R-squared called **adjusted R-squared** that penalizes model complexity.\n",
    "\n",
    "<img src=\"images/regres.png\"  width=\"400\">\n",
    "<img src=\"images/MLR_r2.png\"  width=\"400\">\n",
    "<img src=\"images/adjr2.png\"  width=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information-theoretic Model Selection (AIC and BIC)\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Understand that both AIC and BIC are information criteria that also can be used to perform model selection\n",
    "- Understand how AIC and BIC use likelihood\n",
    "\n",
    "\n",
    "Information criteria (IC) choose models by trading off model fit against a penalty\n",
    "for model complexity as measured by the number of free parameters that have to\n",
    "be estimated for the models. Several information criteria have been suggested in\n",
    "the literature, the two most popular of which are the Bayes information criterion\n",
    "(Schwarz BIC, SBIC, or BIC) proposed by Schwarz (1978) and the Akaike information\n",
    "criterion (AIC) due to Akaike (1974)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information criteria employ different strategies to trade-off fit against parsimony.\n",
    "The Bayesian information criterion selects the model with the highest posterior\n",
    "probability given the data. To choose a single model from the candidate set, the\n",
    "models are ranked according to their posterior probabilities and the model with the\n",
    "highest posterior probability is chosen. The Akaike information criterion seeks to minimize the (Kullback–Leibler) distance between the density of a candidate model\n",
    "and the density of the true (unknown) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The AIC\n",
    "\n",
    "One approach to model selection is to use an information-theoretic criterion to identify the most appropriate model. Akaike (1973) found a formal relationship between Kullback-Leibler information (a dominant paradigm in information and coding theory) and likelihood theory. Akaike's Information Criterion (AIC) is an estimator of expected relative K-L information based on the maximized log-likelihood function, corrected for asymptotic bias. \n",
    "\n",
    "<br>\n",
    "<center> AIC(model) = -2  $\\cdot$ log-likelihood(model) + 2 $\\cdot$ (length of the parameter space)</center>\n",
    "\n",
    "$$\\text{AIC} = −2 \\log(L(\\theta|data)) + 2p$$\n",
    "\n",
    "AIC balances the fit of the model (in terms of the likelihood) with the number of parameters required to achieve that fit. We can easily calculate AIC from the residual sums of squares as:\n",
    "\n",
    "$$\\text{AIC} = n \\log(\\text{RSS}/n) + 2p$$\n",
    "\n",
    "where $p$ is the number of parameters in the model. Notice that as the number of parameters increase, the residual sum of squares goes down, but the second term (a penalty) increases.\n",
    "\n",
    "To apply AIC to a model selection problem, we choose the model that has the lowest AIC value.\n",
    "\n",
    "The AIC is generally used to compare each candidate model. The nice thing about the AIC is that for every model that uses Maximum Likelihood Estimation, the log-likelihood is automatically computed, and as a consequence the AIC is very easy to calculate.\n",
    "The AIC acts as a penalised log-likelihood criterion, giving a balance between a good fit (high value of log-likelihood) and complexity (complex models are penalized more than fairly simple ones). The AIC is unbounded so can take any type of value, but the bottom line is that when comparing models, the model with the lowest AIC should be selected.\n",
    "Note that directly comparing the values of log-likelihood maxima for different models (without including the penalty) is not good enough for model comparison, because including more parameters in a model will always give rise to an increased value of the maximum likelihood. Because of that reason, searching for the model with maximal log-likelihood would always lead to the model with the most parameters. The AIC balances this by penalizing for number of parameters, hence searching for models with few parameters but fitting the data well. data well.\n",
    "\n",
    "> It makes rather stringent assumptions about the form of the likelihood, and so cannot be universally applied.\n",
    "\n",
    "#### The BIC\n",
    "\n",
    "The BIC (Bayesian Information Criterion) is very similar to the AIC and emerged as a Bayesian response to the AIC, but can be used for the exact same purposes. The idea is to select the candidate model with the highest probability given the data. This idea can be formalised inside a Bayesian framework, involving prior probabilities on candidate models along with prior densities on all parameters in the models. The penalty is slightly changed and depends on the number of rows to the data set:\n",
    "\n",
    "<br>\n",
    "<center> BIC(model) = -2 $\\cdot$ log-likelihood(model) + log(number of observations) $\\cdot$ (length of the parameter space)</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So is there a better approach to feature selection? **Cross-validation.** It provides a more reliable estimate of out-of-sample error, and thus is a better way to choose which of your models will best **generalize** to out-of-sample data. Importantly, cross-validation can be applied to any model, whereas the methods described above only apply to linear models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
