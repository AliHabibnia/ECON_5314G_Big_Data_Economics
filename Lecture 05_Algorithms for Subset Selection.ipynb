{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font color=navy>Big Data Economics</font></center>\n",
    "### <center>Algorithms for Subset Selection in Multiple Linear Regression</center>\n",
    "#### <center>Ali Habibnia</center>\n",
    "    \n",
    "<center> Assistant Professor, Department of Economics, </center>\n",
    "<center> and Division of Computational Modeling & Data Analytics at Virginia Tech</center>\n",
    "<center> habibnia@vt.edu </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readings:\n",
    "\n",
    "1. ***Chapter 6.2,*** Graham Elliott, and Allan Timmermann, Economic Forecasting, Princeton University Press, 2016.\n",
    "2. ***Chapter 3.3*** [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals\n",
    "\n",
    "> In this note, we introduce methods for identifying subsets of the independent variables to improve predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For moderate numbers of independent variables the method discussed during previous session is preferable. The idea was to evaluate all subsets. \n",
    "\n",
    "- We compute a criterion such as adjusted $R^2$. We use the adjusted $R^2$ for all subsets to choose the best one. \n",
    "\n",
    "- This is only feasible if $p$ is less than about 20\n",
    "\n",
    "- Since the number of subsets for even moderate values of $p$ is very large, we need some way to examine the most promising subsets and to select from them. An intuitive metric to compare subsets is adjusted $R^2$.\n",
    "\n",
    "- A criterion that is often used for subset selection is known as Mallow‚Äôs $C_p$. This criterion assumes that the full model is unbiased although it may have variables that, if dropped, would improve the mean squared error (MSE). \n",
    "\n",
    "- $C_p$ is also an estimate of the sum of MSE (standardized by dividing by œÉ2) for predictions (the fitted values) at the x-values observed in the training set. Thus good models are those that have values of $C_p$ near k and that have small k (i.e. are of small size). $C_p$ is computed from the formula:\n",
    "\n",
    "$$\n",
    "C_p = \\frac{SSR}{\\hat{\\sigma}^2_{Full}} + 2k - n\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $SSR$ is the sum of squares Residualls due to regression.\n",
    "- $\\hat{\\sigma}^2_{Full}$ is an estimate of the variance associated with the full model.\n",
    "- $k$ is the number of predictors in the model.\n",
    "- $n$ is the sample size.\n",
    "\n",
    "\n",
    "\n",
    "- It is important to remember that the usefulness of this approach depends heavily on the reliability of the estimate of ${\\sigma}^2$ for the full model. This requires that the training set contains a large number of observations relative to the number of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset Selection in Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A frequent problem in data mining is that of using a regression equation to predict the value of a dependent variable when we have a number of variables available to choose as independent variables in our model. \n",
    "\n",
    "- Given the high speed of modern algorithms for multiple linear regression calculations, it is tempting in such a situation to take a kitchen-sink approach: why bother to select a subset, just use all the variables in the model. \n",
    "\n",
    "- There are several reasons why this could be undesirable.\n",
    "\n",
    "    - It may be expensive to collect the full complement of variables for future predictions.\n",
    "    - We may be able to more accurately measure fewer variables (for example in surveys).\n",
    "    - Parsimony is an important property of good models. We obtain more insight into the influence of regressors in models with a few parameters.\n",
    "    - Estimates of regression coefficients are likely to be unstable due to multicollinearity in models with many variables. We get better insights into the influence of regressors from models with fewer variables as the coefficients are more stable for parsimonious models.\n",
    "    - It can be shown that using independent variables that are uncorrelated with the dependent variable will increase the variance of predictions.\n",
    "    - It can be shown that dropping independent variables that have small (non-zero) coefficients can reduce the average error of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms for Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Selecting subsets to improve MSE is a difficult computational problem for large number $p$ of independent variables. \n",
    "\n",
    "- The most common procedure for $p$ greater than about 20 is to use heuristics to select ‚Äúgood‚Äù subsets rather than to lookfor the best subset for a given criterion. \n",
    "\n",
    "- The heuristics most often used and available in statistics software are step-wise procedures. \n",
    "\n",
    "- There are three common procedures: **forward selection**, **backward elimination** and **step-wise regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Selection\n",
    "\n",
    "**Forward selection** is a stepwise approach that starts with no variables in the model (the null model) and adds them one by one. In the first step, it adds the most significant variable. At each subsequent step, it adds the most significant variable of those not in the model, until there are no variables that meet the criterion set by the researcher.\n",
    "\n",
    "### Algorithm:\n",
    "\n",
    "1. Start with a model containing only the constant term (intercept).\n",
    "2. For each variable not in the model:\n",
    "   - Compute the decrease in SSR (Sum of Squares of the Residuals) that would result from adding the variable.\n",
    "   - Estimate $\\hat{\\sigma}^2$ for the current model.\n",
    "   - Calculate the F-statistic:\n",
    "     $$\n",
    "     F_i = \\frac{SSR(S) - SSR(S \\cup \\{i\\})}{\\hat{\\sigma}^2(S \\cup \\{i\\})}\n",
    "     $$\n",
    "   - If $F_i$ is greater than a predefined threshold $F_{in}$ (often between 2 and 4), add variable $i$ to the model.\n",
    "3. Repeat step 2 until no more variables meet the criterion for inclusion.\n",
    "\n",
    "\n",
    "\n",
    "## Backward Elimination\n",
    "\n",
    "**Backward elimination** is the reverse process of forward selection. We start with all candidate variables (full model) and remove the least significant variable at each step, until none meet the criterion.\n",
    "\n",
    "### Algorithm:\n",
    "1. Start with all variables in the model.\n",
    "2. For each variable in the model:\n",
    "   - Compute the increase in SSR that would result from removing the variable.\n",
    "   - Estimate $\\hat{\\sigma}^2$ for the current model.\n",
    "   - Calculate the F-statistic:\n",
    "     $$\n",
    "     F_i = \\frac{SSR(S - \\{i\\}) - SSR(S)}{\\hat{\\sigma}^2(S)}\n",
    "     $$\n",
    "   - If $F_i$ is less than a predefined threshold $F_{out}$ (often between 2 and 4), remove variable $i$ from the model.\n",
    "3. Repeat step 2 until no more variables meet the criterion for removal.\n",
    "\n",
    "- Backward Elimination has the advantage that all variables are included in $S$ at some stage. \n",
    "\n",
    "- This addresses a problem of forward selection that will never select a variable that is better than a previously selected variable that is strongly correlated with it. \n",
    "\n",
    "- The disadvantage is that the full model with all variables is required at the start and this can be time-consuming and numerically unstable.\n",
    "\n",
    "## Stepwise Regression\n",
    "\n",
    "**Stepwise regression** alternates between forward and backward, bringing in and removing variables that meet the criteria for entry or removal, until a stable set of variables is attained. This procedure is like Forward Selection except that at each step we consider dropping variables as in Backward Elimination.\n",
    "\n",
    "\n",
    "### Algorithm:\n",
    "1. Start with no variables in the model or all variables, depending on the specific type of stepwise regression.\n",
    "2. Perform steps from forward selection to add significant variables.\n",
    "3. Perform steps from backward elimination to remove non-significant variables.\n",
    "4. Repeat steps 2 and 3 until adding or removing variables does not improve the model.\n",
    "\n",
    "- Convergence is guaranteed if the thresholds $F_{out}$ and $F_{in}$ satisfy: $F_{out}$ < $F_{in}$. It is possible, however, for a variable to enter $S$ and then leave $S$ at a subsequent step and even rejoin $S$ at a yet later step.\n",
    "\n",
    "- These methods pick one best subset. There are straightforward variations of the methods that do identify several close to best choices for different sizes of independent variable subsets.\n",
    "\n",
    "- Each of these methods has its own merits and can be chosen based on the specific needs of the data and the analysis being performed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The essential problems with stepwise methods can be paraphrased as follows:\n",
    "1. $R^2$ values are biased high\n",
    "2. The $F$ statistics do not have the claimed distribution.\n",
    "3. The standard errors of the parameter estimates are too small.\n",
    "4. Consequently, the confidence intervals around the parameter estimates are too narrow.\n",
    "5. p-values are too low, due to multiple comparisons, and are difficult to correct.\n",
    "6. Parameter estimates are biased away from 0.\n",
    "7. Collinearity problems are exacerbated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset Selection in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. f_regression in sklearn. See [1](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html), [2](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)\n",
    "> Scikit-learn indeed does not support stepwise regression. That's because what is commonly known as 'stepwise regression' is an algorithm based on p-values of coefficients of linear regression, and scikit-learn deliberately avoids inferential approach to model learning (significance testing etc).\n",
    "2. Forward Selection by adjusted $ùëÖ^2$ with statsmodels. See [link](https://planspace.org/20150423-forward_selection_with_statsmodels/)\n",
    "3. [stepwise-regression package](https://pypi.org/project/stepwise-regression/#description) which executes linear regression forward and backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install stepwise-regression> Null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boston Housing Dataset consists of price of houses in various places in Boston. Alongside with price, the dataset also provide information such as Crime (CRIM), areas of non-retail business in the town (INDUS), the age of people who own the house (AGE), and there are many other attributes that available in the dataset.\n",
    "\n",
    "\n",
    "**The objective is to predict the value of prices of the house using the given features.**\n",
    "\n",
    "- Number of Instances: 506\n",
    "\n",
    "- Number of Attributes: 13 continuous attributes and 1 binary-valued attribute.\n",
    "\n",
    "- Attribute Information:\n",
    "\n",
    "    1. CRIM      per capita crime rate by town\n",
    "    2. ZN        proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "    3. INDUS     proportion of non-retail business acres per town\n",
    "    4. CHAS      Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "    5. NOX       nitric oxides concentration (parts per 10 million)\n",
    "    6. RM        average number of rooms per dwelling\n",
    "    7. AGE       proportion of owner-occupied units built prior to 1940\n",
    "    8. DIS       weighted distances to five Boston employment centres\n",
    "    9. RAD       index of accessibility to radial highways\n",
    "    10. TAX      full-value property-tax rate per 10,000 Dollars\n",
    "    11. PTRATIO  pupil-teacher ratio by town\n",
    "    12. B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "    13. LSTAT    \\% lower status of the population\n",
    "    14. MEDV     Median value of owner-occupied homes in 1000's Dollars\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Boston housing dataset from the provided URL\n",
    "url = \"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\"\n",
    "boston = pd.read_csv(url)\n",
    "\n",
    "# Separate predictors and response\n",
    "X = boston.drop('medv', axis=1)  # predictors\n",
    "y = boston['medv']  # response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        b  lstat  \n",
       "0  396.90   4.98  \n",
       "1  396.90   9.14  \n",
       "2  392.83   4.03  \n",
       "3  394.63   2.94  \n",
       "4  396.90   5.33  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the stepwise selection functions\n",
    "\n",
    "def forward_selection(X, y, significance_level=0.01):\n",
    "    initial_features = []\n",
    "    best_features = []\n",
    "    while True:\n",
    "        remaining_features = list(set(X.columns) - set(best_features))\n",
    "        new_pval = pd.Series(index=remaining_features, dtype=float)\n",
    "        for new_column in remaining_features:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[best_features + [new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        min_p_value = new_pval.min()\n",
    "        if min_p_value < significance_level:\n",
    "            best_features.append(new_pval.idxmin())\n",
    "        else:\n",
    "            break\n",
    "    return best_features\n",
    "\n",
    "def backward_elimination(X, y, significance_level=0.01):\n",
    "    features = list(X.columns)\n",
    "    while len(features) > 0:\n",
    "        features_with_constant = sm.add_constant(X[features])\n",
    "        p_values = sm.OLS(y, features_with_constant).fit().pvalues[1:]\n",
    "        max_p_value = p_values.max()\n",
    "        if max_p_value > significance_level:\n",
    "            excluded_feature = p_values.idxmax()\n",
    "            features.remove(excluded_feature)\n",
    "        else:\n",
    "            break\n",
    "    return features\n",
    "\n",
    "def stepwise_reg(X, y, significance_level_in=0.01, significance_level_out=0.01):\n",
    "    features_in = []\n",
    "    features_out = list(X.columns)\n",
    "    while True:\n",
    "        remaining_features = list(set(features_out) - set(features_in))\n",
    "        pval_in = pd.Series(index=remaining_features, dtype=float)\n",
    "        for new_column in remaining_features:\n",
    "            model_in = sm.OLS(y, sm.add_constant(pd.DataFrame(X[features_in + [new_column]]))).fit()\n",
    "            pval_in[new_column] = model_in.pvalues[new_column]\n",
    "        min_p_value_in = pval_in.min()\n",
    "        if min_p_value_in < significance_level_in:\n",
    "            features_in.append(pval_in.idxmin())\n",
    "            features_out.remove(pval_in.idxmin())\n",
    "            while True:\n",
    "                model_out = sm.OLS(y, sm.add_constant(pd.DataFrame(X[features_in]))).fit()\n",
    "                p_values_out = model_out.pvalues.iloc[1:]\n",
    "                max_p_value_out = p_values_out.max()\n",
    "                if max_p_value_out > significance_level_out:\n",
    "                    excluded_feature = p_values_out.idxmax()\n",
    "                    features_in.remove(excluded_feature)\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            break\n",
    "    return features_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Forward Selection</th>\n",
       "      <th>Backward Elimination</th>\n",
       "      <th>Stepwise Selection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lstat</td>\n",
       "      <td>crim</td>\n",
       "      <td>lstat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rm</td>\n",
       "      <td>zn</td>\n",
       "      <td>rm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ptratio</td>\n",
       "      <td>chas</td>\n",
       "      <td>ptratio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dis</td>\n",
       "      <td>nox</td>\n",
       "      <td>dis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nox</td>\n",
       "      <td>rm</td>\n",
       "      <td>nox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chas</td>\n",
       "      <td>dis</td>\n",
       "      <td>chas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b</td>\n",
       "      <td>rad</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>zn</td>\n",
       "      <td>tax</td>\n",
       "      <td>zn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>ptratio</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>b</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>lstat</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Forward Selection Backward Elimination Stepwise Selection\n",
       "0              lstat                 crim              lstat\n",
       "1                 rm                   zn                 rm\n",
       "2            ptratio                 chas            ptratio\n",
       "3                dis                  nox                dis\n",
       "4                nox                   rm                nox\n",
       "5               chas                  dis               chas\n",
       "6                  b                  rad                  b\n",
       "7                 zn                  tax                 zn\n",
       "8                                 ptratio                   \n",
       "9                                       b                   \n",
       "10                                  lstat                   "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the stepwise selection methods\n",
    "forward_selected_features = forward_selection(X, y)\n",
    "backward_eliminated_features = backward_elimination(X, y)\n",
    "stepwise_selected_features = stepwise_reg(X, y)\n",
    "\n",
    "# Create a DataFrame to display the features in a table\n",
    "features_df = pd.DataFrame({\n",
    "    'Forward Selection': pd.Series(forward_selected_features),\n",
    "    'Backward Elimination': pd.Series(backward_eliminated_features),\n",
    "    'Stepwise Selection': pd.Series(stepwise_selected_features)\n",
    "})\n",
    "\n",
    "# Fill NaN values with empty strings for a cleaner presentation\n",
    "features_df = features_df.fillna('')\n",
    "features_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
