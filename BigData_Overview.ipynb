{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font color=navy>Big Data Economics</font></center>\n",
    "### <center>An Overview</center>\n",
    "#### <center>Ali Habibnia</center>\n",
    "\n",
    "    \n",
    "<center> Assistant Professor, Department of Economics, </center>\n",
    "<center> and Division of Computational Modeling & Data Analytics at Virginia Tech</center>\n",
    "<center> habibnia@vt.edu </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "<img src=\"images/tech3.jpg\" alt=\"Drawing\" width=\"350\"/>\n",
    "\n",
    "#### The image exemplifies the intersection and collaborative synergy among three pivotal technological domains:\n",
    "\n",
    "1. **Artificial Intelligence and Machine Learning**: AI and ML can leverage Big Alternative Data for developing more sophisticated models and algorithms, which can lead to more accurate predictions and better decision-making.\n",
    "\n",
    "2. **Big Data**: provides the raw information that allows AI models to make informed decisions. This data can be processed and analyzed using machine learning algorithms to uncover insights that were previously not possible with traditional data sources, enhancing research and development across various fields.\n",
    "\n",
    "3. **High-Performance Computing (HPC)**: HPC provides the necessary computational power to handle the vast amounts of data and complex calculations required by AI and ML, thus speeding up research and enabling more complex simulations. \n",
    "\n",
    "The integration of Big Data, AI, and HPC creates a powerful ecosystem for advanced analytics, enabling the tackling of intricate problems and the extraction of profound insights. This triad fosters the capability for real-time analysis and decision-making, revolutionizing sectors such as finance, healthcare, and transportation by improving efficiency and outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### The concept of Big Data is often associated with the (?) V's:\n",
    "\n",
    "<img src=\"images/bigdatavs.jpg\" alt=\"Drawing\" width=\"500\"/>\n",
    "\n",
    "\n",
    "1. **Visualization**: Represents the importance of presenting data in a manner that is easily and immediately understandable.\n",
    "\n",
    "2. **Velocity**: Refers to the speed at which data is generated, processed, and analyzed.\n",
    "\n",
    "3. **Variety**: Indicates the different types of data (structured, unstructured, and semi-structured) that are available for analysis.\n",
    "\n",
    "4. **Variability**: Suggests that data flows can be highly inconsistent with periodic peaks.\n",
    "\n",
    "5. **Volume**: Points to the vast amounts of data generated from various sources.\n",
    "\n",
    "6. **Vulnerability**: Highlights the security concerns and risks associated with managing and storing large quantities of data.\n",
    "\n",
    "7. **Validity**: Concerns the accuracy and correctness of data for the intended use.\n",
    "\n",
    "8. **Volatility**: Describes how long data is valid and how quickly it becomes outdated.\n",
    "\n",
    "9. **Veracity**: Addresses the quality and reliability of data.\n",
    "\n",
    "10. **Value**: Emphasizes the worth of the data being collected and how it can be turned into a valuable resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Big Data?\n",
    "\n",
    "<img src=\"images/model.png\" alt=\"Drawing\"/>\n",
    "\n",
    "Big Data is a term that varies in definition depending on the context and the person you're asking. In the field of econometrics, the concept of Big Data can be framed with respect to the dimensions of the dataset, namely the number of variables and observations.\n",
    "\n",
    "* **Wild data** (unstructured, constract with Census surveys, or Twitter)\n",
    "\n",
    "* **Wide data** (a.k.a Larg-P data because p>>N)\n",
    "\n",
    "* **Long data** (a.k.a Large-N data because N is very large and may not even fit onto a single hard drive)\n",
    "\n",
    "* **Complex model** (a.k.a Large-Theta because model/algorithm has many parameters)\n",
    "\n",
    "<img src=\"images/mlp.png\" alt=\"Drawing\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pillars of Big Data\n",
    "\n",
    "* Foundation of basic calculus, linear algebra, probability analysis, and numerical optimization)\n",
    "\n",
    "* Programming (for automation of data collection, manipulation, cleaning, visualization, and modeling)\n",
    "\n",
    "* Visualization & exploration\n",
    "\n",
    "* Machine learning (to capture nonlinearity and non-normality in data, to compress data, and for prediction)\n",
    "\n",
    "* Causal inference (to be able to make policy prescription)\n"
   ]
  },
  {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
    "## Impact of the Curse of Dimensionality on Regression Analysis\n",
    "\n",
    "Before delving into the specific impacts, we first establish the general equations and notation used in regression analysis.\n",
    "\n",
    "### Linear Regression: Basic Equations and Matrix Formulation\n",
    "\n",
    "Linear regression is one of the foundational models in statistics, econometrics, and machine learning. It provides a simple yet powerful framework for modeling the relationship between a dependent variable and one or more explanatory variables.\n",
    "\n",
    "#### 1. Scalar and Componentwise Representation\n",
    "\n",
    "Consider a dataset with n observations and k explanatory variables. For observation i, the linear regression model is written as\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_k x_{ik} + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $y_i$ is the dependent variable\n",
    "- $x_{ij}$ denotes the j-th explanatory variable\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_j$ for $j = 1, \\ldots, k$ are slope coefficients\n",
    "- $\\varepsilon_i$ is an unobserved error term capturing noise and omitted factors\n",
    "\n",
    "The objective of linear regression is to estimate the parameter vector $(\\beta_0, \\beta_1, \\ldots, \\beta_k)$ such that the fitted values approximate the observed outcomes as closely as possible.\n",
    "\n",
    "#### 2. Vector and Matrix Formulation\n",
    "\n",
    "Stacking all observations together yields a compact matrix representation. Define\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "X =\n",
    "\\begin{pmatrix}\n",
    "1 & x_{11} & \\cdots & x_{1k} \\\\\n",
    "1 & x_{21} & \\cdots & x_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{n1} & \\cdots & x_{nk}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta =\n",
    "\\begin{pmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_k\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\varepsilon =\n",
    "\\begin{pmatrix}\n",
    "\\varepsilon_1 \\\\\n",
    "\\varepsilon_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\varepsilon_n\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The linear regression model can then be written succinctly as\n",
    "\n",
    "$$\n",
    "y = X\\beta + \\varepsilon\n",
    "$$\n",
    "\n",
    "This formulation highlights that linear regression is a linear mapping from the parameter space to the outcome space.\n",
    "\n",
    "#### 3. Ordinary Least Squares Estimation\n",
    "\n",
    "The most common estimation method is Ordinary Least Squares (OLS), which chooses $\\beta$ to minimize the sum of squared residuals\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} (y - X\\beta)'(y - X\\beta)\n",
    "$$\n",
    "\n",
    "Provided that $X'X$ is invertible, the closed-form OLS estimator is\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X'X)^{-1}X'y\n",
    "$$\n",
    "\n",
    "This expression plays a central role in econometrics and statistical learning.\n",
    "\n",
    "#### 4. Interpretation\n",
    "\n",
    "- Each coefficient $\\beta_j$ measures the marginal effect of $x_j$ on the expected value of $y$, holding other regressors fixed\n",
    "- The matrix formulation clarifies issues such as identification, multicollinearity, and numerical stability\n",
    "- Many extensions, including ridge regression, LASSO, generalized linear models, and neural networks, can be viewed as generalizations of this framework\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X}\\beta + \\boldsymbol{\\varepsilon}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{OLS}(\\hat{\\beta}) = \\sum_{i=1}^n (y_i - \\mathbf{x}_i^T \\hat{\\beta})^2 = \\|\\mathbf{y} - \\mathbf{X}\\beta\\|^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{OLS} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Bias}(\\hat{\\beta}_{OLS}) = \\mathbb{E}[\\hat{\\beta}_{OLS}] - \\beta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}_{OLS}) = \\sigma^2 (\\mathbf{X}^T \\mathbf{X})^{-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\frac{\\boldsymbol{\\varepsilon}^T \\boldsymbol{\\varepsilon}}{n - p - 1}\n",
    "$$\n",
    "\n",
    "### Overfitting\n",
    "- In high-dimensional settings where $p$ is large, OLS can fit noise as well as signal.\n",
    "- Overfitting occurs when training error is small but out-of-sample error is large.\n",
    "- When $p \\ge n$, $(\\mathbf{X}^T \\mathbf{X})^{-1}$ becomes unstable or undefined.\n",
    "\n",
    "### Collinearity\n",
    "- Collinearity refers to strong dependence among regressors in $\\mathbf{X}$.\n",
    "- It inflates the variance of $\\hat{\\beta}_{OLS}$ and obscures individual effects.\n",
    "\n",
    "### Sample Size and Power Issues\n",
    "- As $p$ increases, larger $n$ is required to maintain statistical power.\n",
    "- High dimensionality increases both Type I and Type II error risks.\n",
    "\n",
    "### Unreliable Estimation of Variance\n",
    "- When $p$ is close to $n$, $(\\mathbf{X}^T \\mathbf{X})$ becomes poorly conditioned.\n",
    "- This undermines the reliability of variance and inference.\n",
    "\n",
    "### Estimation of Error Variance ($\\sigma^2$)\n",
    "- The estimator depends critically on degrees of freedom $n - p - 1$.\n",
    "- Large $p$ leads to unstable or inflated variance estimates.\n",
    "\n",
    "The curse of dimensionality increases overfitting risk, worsens collinearity, weakens inference, and motivates regularization and dimensionality reduction in modern regression analysis."
  ]
},
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data: FROM HYPOTHESIS TESTING TO MACHINE LEARNING\n",
    "\n",
    "* Machine learning (ML) allows researchers to analyze data in novel ways. Computers today can process multiple sets of data quickly and, with the right classification sets, recognize highly complex patterns. \n",
    "\n",
    "* Designed to simulate the interactions of biological neurons, “deep learning” uses artificial neural networks to discern features in successive layers of data while iterating on previously recognized trends. \n",
    "\n",
    "\n",
    "### Econometrics vs. Machine Learning\n",
    "\n",
    "#### Goal of econometrics\n",
    "\n",
    "* \"the goal of econometrics is to find β hat\" where here we mean β hat to be the causal impact of X on y\n",
    "\n",
    "* The primary statistical concern of econometrics is sampling error. In other words, the goal is to quantify the uncertainty around β hat due to randomness in the sampling of the population. \n",
    "\n",
    "* The goal is to make counterfactual predictions.\n",
    "\n",
    "        What would happen to Amazon's profits if it changed its website layout?\n",
    "\n",
    "We don't get to observe the world under these alternative policies, so we can't simply find the answers in the data. Knowing the counterfactual requires measuring a causal effect. Measuring a causal effect requires making assumptions. That's what economics is all about!\n",
    "\n",
    "\n",
    "#### Goal of machine learning\n",
    "\n",
    "* In contrast, the goal of machine learning is to come up with the best possible out-of-sample prediction. \n",
    "\n",
    "* We refer to this as the primary concern of machine learning being \"y hat\"\n",
    "\n",
    "* The goal is to make sure that the best prediction is made by tuning and validating many different kinds of models. This is what cross-validation is all about, and it is what machine learning practitioners obsess about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlocking New Dimensions: The Pivotal Role of Alternative Data\n",
    "\n",
    "- **Alternative data**: refers to non-traditional data sources that can provide additional insights beyond what's available through conventional data. Its advantages are frequency and near-real-time data, accuracy and objectiveness. Its disadvantages are the fact that the indicators available are merely proxies for what policymakers are interested in and need for policy design. Here are some examples of alternative data and variables:\n",
    "\n",
    "    - **Textual data such as News Headlines**\n",
    "    - **Digital footprints from social media**\n",
    "    - **Mobile phone data**\n",
    "    - **Satellite Imagery** nighttime light measures, or luminosity, as proxies for economic activity and population distribution\n",
    "    - **Search Engine Trends**\n",
    "    - **Environmental, Social, and Governance (ESG) Data**\n",
    "\n",
    "\n",
    "<img src=\"images/text.png\" alt=\"Drawing\" width=\"400\"/>\n",
    "<img src=\"images/satt2.jpg\" alt=\"Drawing\" width=\"400\"/>\n",
    "<img src=\"images/satt.jpg\" alt=\"Drawing\" width=\"400\"/>\n",
    "<img src=\"images/satt3.jpg\" alt=\"Drawing\" width=\"400\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
