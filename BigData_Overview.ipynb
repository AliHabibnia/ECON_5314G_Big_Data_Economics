{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font color=navy>Big Data Economics</font></center>\n",
    "### <center>An Overview</center>\n",
    "#### <center>Ali Habibnia</center>\n",
    "\n",
    "    \n",
    "<center> Assistant Professor, Department of Economics, </center>\n",
    "<center> and Division of Computational Modeling & Data Analytics at Virginia Tech</center>\n",
    "<center> habibnia@vt.edu </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "<img src=\"images/tech3.jpg\" alt=\"Drawing\" width=\"350\"/>\n",
    "\n",
    "#### The image exemplifies the intersection and collaborative synergy among three pivotal technological domains:\n",
    "\n",
    "1. **Artificial Intelligence and Machine Learning**: AI and ML can leverage Big Alternative Data for developing more sophisticated models and algorithms, which can lead to more accurate predictions and better decision-making.\n",
    "\n",
    "2. **Big Data**: provides the raw information that allows AI models to make informed decisions. This data can be processed and analyzed using machine learning algorithms to uncover insights that were previously not possible with traditional data sources, enhancing research and development across various fields.\n",
    "\n",
    "3. **High-Performance Computing (HPC)**: HPC provides the necessary computational power to handle the vast amounts of data and complex calculations required by AI and ML, thus speeding up research and enabling more complex simulations. \n",
    "\n",
    "The integration of Big Data, AI, and HPC creates a powerful ecosystem for advanced analytics, enabling the tackling of intricate problems and the extraction of profound insights. This triad fosters the capability for real-time analysis and decision-making, revolutionizing sectors such as finance, healthcare, and transportation by improving efficiency and outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### The concept of Big Data is often associated with the (?) V's:\n",
    "\n",
    "<img src=\"images/bigdatavs.jpg\" alt=\"Drawing\" width=\"500\"/>\n",
    "\n",
    "\n",
    "1. **Visualization**: Represents the importance of presenting data in a manner that is easily and immediately understandable.\n",
    "\n",
    "2. **Velocity**: Refers to the speed at which data is generated, processed, and analyzed.\n",
    "\n",
    "3. **Variety**: Indicates the different types of data (structured, unstructured, and semi-structured) that are available for analysis.\n",
    "\n",
    "4. **Variability**: Suggests that data flows can be highly inconsistent with periodic peaks.\n",
    "\n",
    "5. **Volume**: Points to the vast amounts of data generated from various sources.\n",
    "\n",
    "6. **Vulnerability**: Highlights the security concerns and risks associated with managing and storing large quantities of data.\n",
    "\n",
    "7. **Validity**: Concerns the accuracy and correctness of data for the intended use.\n",
    "\n",
    "8. **Volatility**: Describes how long data is valid and how quickly it becomes outdated.\n",
    "\n",
    "9. **Veracity**: Addresses the quality and reliability of data.\n",
    "\n",
    "10. **Value**: Emphasizes the worth of the data being collected and how it can be turned into a valuable resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Big Data?\n",
    "\n",
    "<img src=\"images/model.png\" alt=\"Drawing\"/>\n",
    "\n",
    "Big Data is a term that varies in definition depending on the context and the person you're asking. In the field of econometrics, the concept of Big Data can be framed with respect to the dimensions of the dataset, namely the number of variables and observations.\n",
    "\n",
    "* **Wild data** (unstructured, constract with Census surveys, or twitter)\n",
    "\n",
    "* **Wide data** (a.k.a Larg-P data because p>>N)\n",
    "\n",
    "* **Long data** (a.k.a Large-N data because N is very large and may not even fit onto a single hard drive)\n",
    "\n",
    "* **Complex model** (a.k.a Large-Theta because model/algorithm has many parameters)\n",
    "\n",
    "<img src=\"images/mlp.png\" alt=\"Drawing\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pillars of Big Data\n",
    "\n",
    "* Foundation of basic calculus, linear algebra, probability analysis, and neumerical optimization)\n",
    "\n",
    "* Programming (for automation of data collection, manipulation, cleaning, visualization, and modeling)\n",
    "\n",
    "* Visualization & exploration\n",
    "\n",
    "* Machine learning (to capture nonlinearity and non normality in data, to compress data, and prediction)\n",
    "\n",
    "* Causal inference (to be able to make policy prescription)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of the Curse of Dimensionality on Regression Analysis\n",
    "\n",
    "Before delving into the specific impacts, let's establish the general equations and notations used in regression analysis:\n",
    "\n",
    "### Linear Regression: Basic Equations and Matrix Formulation\n",
    "\n",
    "Linear regression is one of the foundational models in statistics, econometrics, and machine learning. It provides a simple yet powerful framework for modeling the relationship between a dependent variable and one or more explanatory variables.\n",
    "\n",
    "#### 1. Scalar and Componentwise Representation\n",
    "\n",
    "Consider a dataset with n observations and k explanatory variables. For observation i, the linear regression model is written as\n",
    "\n",
    "<br>\n",
    "<center> $y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_k x_{ik} + \\varepsilon_i$ </center>\n",
    "<br>\n",
    "where\n",
    "\n",
    "* $y_i$ is the dependent variable\n",
    "* $x_{ij}$ denotes the j-th explanatory variable\n",
    "* $\\beta_0$ is the intercept\n",
    "* $\\beta_j$ for j = 1,…,k are slope coefficients\n",
    "* $\\varepsilon_i$ is an unobserved error term capturing noise and omitted factors\n",
    "\n",
    "The objective of linear regression is to estimate the parameter vector $(\\beta_0, \\beta_1, \\ldots, \\beta_k)$ such that the fitted values approximate the observed outcomes as closely as possible.\n",
    "\n",
    "#### 2. Vector and Matrix Formulation\n",
    "\n",
    "Stacking all observations together yields a compact matrix representation. Define\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X}\\beta + \\boldsymbol{\\varepsilon}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{y} \\in \\mathbb{R}^n,\n",
    "\\qquad\n",
    "\\mathbf{X} \\in \\mathbb{R}^{n \\times p},\n",
    "\\qquad\n",
    "\\beta \\in \\mathbb{R}^p,\n",
    "\\qquad\n",
    "\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n\n",
    "$$\n",
    "\n",
    "$y =\n",
    "\\begin{pmatrix}\n",
    "y_1 \\\n",
    "y_2 \\\n",
    "\\vdots \\\n",
    "y_n\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "X =\n",
    "\\begin{pmatrix}\n",
    "1 & x_{11} & \\cdots & x_{1k} \\\n",
    "1 & x_{21} & \\cdots & x_{2k} \\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\n",
    "1 & x_{n1} & \\cdots & x_{nk}\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\beta =\n",
    "\\begin{pmatrix}\n",
    "\\beta_0 \\\n",
    "\\beta_1 \\\n",
    "\\vdots \\\n",
    "\\beta_k\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\varepsilon =\n",
    "\\begin{pmatrix}\n",
    "\\varepsilon_1 \\\n",
    "\\varepsilon_2 \\\n",
    "\\vdots \\\n",
    "\\varepsilon_n\n",
    "\\end{pmatrix}$\n",
    "\n",
    "A standard econometric formulation imposes the following assumptions.\n",
    "\n",
    "Conditional mean zero (exogeneity):\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\varepsilon} \\mid \\mathbf{X}) = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "Homoskedasticity and no cross-sectional correlation:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\boldsymbol{\\varepsilon} \\mid \\mathbf{X}) = \\sigma^2 \\mathbf{I}_n\n",
    "$$\n",
    "\n",
    "Full column rank (existence and identification):\n",
    "\n",
    "$$\n",
    "\\operatorname{rank}(\\mathbf{X}) = p\n",
    "$$\n",
    "\n",
    "For exact finite sample normal theory inference, one may additionally assume i.i.d. Gaussian errors conditional on the regressors:\n",
    "\n",
    "$$\n",
    "\\varepsilon_i \\mid \\mathbf{X} \\sim \\mathcal{N}(0,\\sigma^2),\n",
    "\\qquad\n",
    "\\varepsilon_i \\;\\perp\\!\\!\\!\\perp\\; \\varepsilon_{i'}\n",
    "\\quad\n",
    "\\text{for } i \\neq i'\n",
    "$$\n",
    "\n",
    "Equivalently, in vector notation:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\varepsilon} \\mid \\mathbf{X}\n",
    "\\sim\n",
    "\\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)\n",
    "$$\n",
    "\n",
    "In most econometric applications, exact normality is not required; the moment conditions above suffice for consistency and asymptotic inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS Objective Function, Estimator, and Bias–Variance Properties\n",
    "\n",
    "The ordinary least squares estimator minimizes the quadratic loss\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{OLS}\n",
    "=\n",
    "\\arg\\min_{\\beta \\in \\mathbb{R}^p}\n",
    "L_n(\\beta)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "L_n(\\beta)\n",
    "=\n",
    "(\\mathbf{y} - \\mathbf{X}\\beta)^\\top (\\mathbf{y} - \\mathbf{X}\\beta)\n",
    "=\n",
    "\\|\\mathbf{y} - \\mathbf{X}\\beta\\|^2\n",
    "$$\n",
    "\n",
    "The first order condition yields the closed form solution\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{OLS}\n",
    "=\n",
    "(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "Define the residual vector\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\varepsilon}}\n",
    "=\n",
    "\\mathbf{y} - \\mathbf{X}\\hat{\\beta}_{OLS}\n",
    "$$\n",
    "\n",
    "Under exogeneity, the estimator is unbiased:\n",
    "\n",
    "$$\n",
    "\\text{Bias}(\\hat{\\beta}_{OLS})\n",
    "=\n",
    "\\mathbb{E}(\\hat{\\beta}_{OLS} \\mid \\mathbf{X}) - \\beta\n",
    "=\n",
    "\\mathbf{0}\n",
    "$$\n",
    "\n",
    "The conditional variance of the estimator is\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\beta}_{OLS} \\mid \\mathbf{X})\n",
    "=\n",
    "(\\mathbf{X}^\\top \\mathbf{X})^{-1}\n",
    "\\mathbf{X}^\\top\n",
    "\\operatorname{Var}(\\mathbf{y} \\mid \\mathbf{X})\n",
    "\\mathbf{X}\n",
    "(\\mathbf{X}^\\top \\mathbf{X})^{-1}\n",
    "$$\n",
    "\n",
    "Since\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\mathbf{y} \\mid \\mathbf{X})\n",
    "=\n",
    "\\operatorname{Var}(\\boldsymbol{\\varepsilon} \\mid \\mathbf{X})\n",
    "=\n",
    "\\sigma^2 \\mathbf{I}_n\n",
    "$$\n",
    "\n",
    "we obtain\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\beta}_{OLS} \\mid \\mathbf{X})\n",
    "=\n",
    "\\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2\n",
    "=\n",
    "\\frac{\\hat{\\boldsymbol{\\varepsilon}}^\\top \\hat{\\boldsymbol{\\varepsilon}}}{n - p - 1}\n",
    "$$\n",
    "\n",
    "- The accurate estimation of $\\sigma^2$ is crucial for inference. The formula shows the dependency on $n$ and $p$.\n",
    "- When $p$ is large relative to $n$, the denominator $n-p-1$ becomes small, leading to an overestimation of $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Sample Size and Dimensionality on Estimator Variance\n",
    "\n",
    "The curse of dimensionality significantly impacts regression analysis by increasing the potential for overfitting, causing issues with collinearity, necessitating larger sample sizes for statistical power, and leading to unreliable estimation of the variance of the OLS estimator and error variance $\\sigma^2$. Techniques like regularization, dimensionality reduction, and careful model selection are crucial in high-dimensional settings.\n",
    "\n",
    "Write\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^\\top \\mathbf{X}\n",
    "=\n",
    "n \\left(\\frac{\\mathbf{X}^\\top \\mathbf{X}}{n}\\right)\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\beta}_{OLS} \\mid \\mathbf{X})\n",
    "=\n",
    "\\frac{\\sigma^2}{n}\n",
    "\\left(\\frac{\\mathbf{X}^\\top \\mathbf{X}}{n}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "If $p$ is fixed and\n",
    "\n",
    "$$\n",
    "\\frac{\\mathbf{X}^\\top \\mathbf{X}}{n}\n",
    "\\to\n",
    "\\mathbf{\\Sigma}\n",
    "\\quad \\text{with } \\mathbf{\\Sigma} \\text{ positive definite}\n",
    "$$\n",
    "\n",
    "the variance shrinks at rate $1/n$.\n",
    "\n",
    "As $p$ increases, eigenvalues of $(\\mathbf{X}^\\top \\mathbf{X})/n$ may approach zero, inflating the inverse and increasing estimator variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large-$p$ Pathologies in OLS\n",
    "\n",
    "### Non-existence and interpolation\n",
    "\n",
    "When\n",
    "\n",
    "$$\n",
    "p \\ge n\n",
    "$$\n",
    "\n",
    "the matrix $\\mathbf{X}^\\top \\mathbf{X}$ is singular, and the OLS estimator is not uniquely defined. Even when solutions exist, the model can interpolate the data, achieving near-zero in-sample loss with poor out-of-sample performance.\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "The in-sample loss at the OLS solution is\n",
    "\n",
    "$$\n",
    "L_n(\\hat{\\beta}_{OLS})\n",
    "=\n",
    "\\|\\mathbf{y} - \\mathbf{X}\\hat{\\beta}_{OLS}\\|^2\n",
    "$$\n",
    "\n",
    "Overfitting occurs when this quantity is small, while the expected out-of-sample prediction error\n",
    "- When $p$ approaches or exceeds $n$, $(\\mathbf{X}^T\\mathbf{X})^{-1}$ can become unstable or non-invertible, leading to a model that perfectly fits the training data but fails to generalize.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\n",
    "\\left[\n",
    "(y_0 - x_0^\\top \\hat{\\beta}_{OLS})^2\n",
    "\\mid \\mathbf{X}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "is large.\n",
    "\n",
    "### Collinearity\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^\\top \\mathbf{X}\n",
    "=\n",
    "\\mathbf{U}\\Lambda\\mathbf{U}^\\top\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "(\\mathbf{X}^\\top \\mathbf{X})^{-1}\n",
    "=\n",
    "\\mathbf{U}\\Lambda^{-1}\\mathbf{U}^\\top\n",
    "$$\n",
    "\n",
    "Small eigenvalues in $\\Lambda$ lead to large entries in $\\Lambda^{-1}$, inflating\n",
    "\n",
    "- Collinearity refers to high linear dependency among predictor variables in the matrix $\\mathbf{X}$.\n",
    "- This leads to instability in $(\\mathbf{X}^T\\mathbf{X})^{-1}$, reflected in inflated values in the diagonal of $\\text{Var}(\\hat{\\beta}_{OLS})$, indicating high variance of the coefficient estimates.\n",
    "- Collinearity can make it difficult to discern the individual impact of predictors on $\\mathbf{y}$.\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\beta}_{OLS} \\mid \\mathbf{X})\n",
    "=\n",
    "\\sigma^2 \\mathbf{U}\\Lambda^{-1}\\mathbf{U}^\\top\n",
    "$$\n",
    "\n",
    "### Unreliable estimation of error variance\n",
    "\n",
    "The classical estimator of $\\sigma^2$ is\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2\n",
    "=\n",
    "\\frac{\\hat{\\boldsymbol{\\varepsilon}}^\\top \\hat{\\boldsymbol{\\varepsilon}}}{n - p - 1}\n",
    "$$\n",
    "\n",
    "As $p$ grows, the degrees-of-freedom adjustment shrinks, causing instability in variance estimation and inference.\n",
    "\n",
    "- In high-dimensional spaces, $\\text{Var}(\\hat{\\beta}_{OLS}) = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}$ can become unreliable due to the increased variance of the estimator.\n",
    "- When $p$ is close to or greater than $n$, the matrix $(\\mathbf{X}^T\\mathbf{X})$ is often of low rank and thus non-invertible or poorly conditioned, which significantly affects the reliability of the OLS estimator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing, Type I and Type II Errors, and Power\n",
    "\n",
    "For inference on $\\beta_j$, consider\n",
    "\n",
    "$$\n",
    "H_0 : \\beta_j = \\beta_j^0\n",
    "$$\n",
    "\n",
    "The test statistic is\n",
    "\n",
    "$$\n",
    "T_{n,j}\n",
    "=\n",
    "\\frac{\\hat{\\beta}_j - \\beta_j^0}\n",
    "{\\sqrt{\\hat{\\sigma}^2 [(\\mathbf{X}^\\top \\mathbf{X})^{-1}]_{jj}}}\n",
    "$$\n",
    "\n",
    "Under fixed $p$,\n",
    "\n",
    "$$\n",
    "T_{n,j} \\xrightarrow{d} \\mathcal{N}(0,1)\n",
    "$$\n",
    "\n",
    "The null is rejected when\n",
    "\n",
    "$$\n",
    "|T_{n,j}| > z_{1-\\alpha/2}\n",
    "$$\n",
    "\n",
    "Type I error satisfies\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\text{reject } H_0 \\mid H_0) \\to \\alpha\n",
    "$$\n",
    "\n",
    "Type II error is\n",
    "\n",
    "$$\n",
    "\\beta_n\n",
    "=\n",
    "\\mathbb{P}(\\text{fail to reject } H_0 \\mid H_1)\n",
    "$$\n",
    "\n",
    "Power is\n",
    "\n",
    "$$\n",
    "\\text{Power}_n = 1 - \\beta_n\n",
    "$$\n",
    "\n",
    "in terms of probability\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\text{reject } H_0 \\mid H_1) \\to \\alpha\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\text{reject } H_0 \\mid \\beta_j = \\beta^0_j + \\Delta) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a fixed alternative\n",
    "\n",
    "$$\n",
    "\\beta_j = \\beta_j^0 + \\Delta\n",
    "$$\n",
    "\n",
    "$$\n",
    "T_{n,j}\n",
    "\\xrightarrow{d}\n",
    "\\mathcal{N}\n",
    "\\left(\n",
    "\\frac{\\sqrt{n}\\Delta}\n",
    "{\\sigma \\sqrt{[(\\mathbf{X}^\\top \\mathbf{X}/n)^{-1}]_{jj}}},\n",
    "1\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "implying\n",
    "\n",
    "$$\n",
    "\\beta_n \\to 0\n",
    "\\quad \\text{and} \\quad\n",
    "\\text{Power}_n \\to 1\n",
    "$$\n",
    "\n",
    "For local alternatives\n",
    "\n",
    "$$\n",
    "\\beta_j = \\beta_j^0 + \\frac{h}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "T_{n,j}\n",
    "\\xrightarrow{d}\n",
    "\\mathcal{N}\n",
    "\\left(\n",
    "\\frac{h}\n",
    "{\\sigma \\sqrt{[(\\mathbf{X}^\\top \\mathbf{X}/n)^{-1}]_{jj}}},\n",
    "1\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Sample Size, Significance, and Causal Interpretability\n",
    "\n",
    "- Adequate sample size ($n$) is essential for reliable estimation. However, as $p$ increases, more data is required to maintain statistical power.\n",
    "- The power of a test is the probability that the test correctly rejects a false null hypothesis (Type II error). In high dimensions, the power to detect true effects is diminished due to the increased variance in coefficient estimates.\n",
    "\n",
    "The significance level controls false rejections:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\text{reject } H_0 \\mid H_0) = \\alpha\n",
    "$$\n",
    "\n",
    "Increasing $n$ does not reduce $\\alpha$; it reduces standard errors through\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\beta}_{OLS} \\mid \\mathbf{X})\n",
    "=\n",
    "\\frac{\\sigma^2}{n}\n",
    "\\left(\\frac{\\mathbf{X}^\\top \\mathbf{X}}{n}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "Thus, arbitrarily small effects become statistically detectable.\n",
    "\n",
    "Causal interpretation, however, depends on identification. If\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\varepsilon} \\mid \\mathbf{X}) \\neq \\mathbf{0}\n",
    "$$\n",
    "\n",
    "then $\\hat{\\beta}_{OLS}$ converges to a biased estimand regardless of sample size.\n",
    "\n",
    "\n",
    "### Summary Implication\n",
    "\n",
    "Large $n$ increases detectability but not credibility.  \n",
    "Large $p$ inflates variance and destabilizes inference.  \n",
    "Their interaction motivates regularization and modern high-dimensional econometrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data: FROM HYPOTHESIS TESTING TO MACHINE LEARNING\n",
    "\n",
    "* Machine learning (ML) allows researchers to analyze data in novel ways. Computers today can process multiple sets of data in little time and, with the correct classification sets, recognize highly complex patterns among them. \n",
    "\n",
    "* Designed to simulate the interactions of biological neurons, “deep learning” uses artificial neural networks to discern features in successive layers of data while iterating on previously recognized trends. \n",
    "\n",
    "\n",
    "### Econometrics vs. Machine Learning\n",
    "\n",
    "#### Goal of econometrics\n",
    "\n",
    "* \"the goal of econometrics is to find β hat\" where here we mean β hat to be the causal impact of X on y\n",
    "\n",
    "* The primary statistical concern of econometrics is sampling error. In other words, the goal is to quantify the uncertainty around β hat due to randomness in the sampling of the population. \n",
    "\n",
    "* The goal is to make counterfactual predictions.\n",
    "\n",
    "        What would happen to Amazon's profits if it changed its website layout?\n",
    "\n",
    "We don't get to observe the world under these alternative policies, so we can't simply find the answers in the data. Knowing the counterfactual requires being able to measure a causal effect. Being able to measure a causal effect requires making assumptions. That's what economics is all about!\n",
    "\n",
    "\n",
    "#### Goal of machine learning\n",
    "\n",
    "* In contrast, the goal of machine learning is to come up with the best possible out-of-sample prediction. \n",
    "\n",
    "* We refer to this as the primary concern of machine learning being \"y hat\"\n",
    "\n",
    "* The goal is to make sure that the best prediction is made by tuning and validating many different kinds of models. This is what cross-validation is all about, and it is what machine learning practitioners obsess about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlocking New Dimensions: The Pivotal Role of Alternative Data\n",
    "\n",
    "- **Alternative data**: refers to non-traditional data sources that can provide additional insights beyond what's available through conventional data. Its advantages are frequency and near-real-time data, accuracy and objectiveness. Its disadvantages are the fact that the indicators available are merely proxies for what policymakers are interested in and need for policy design. Here are some examples of alternative data and variables:\n",
    "\n",
    "    - **Textual data such as News Headlines**\n",
    "    - **Digital footprints from social media**\n",
    "    - **Mobile phone data**\n",
    "    - **Satellite Imagery** nighttime light measures, or luminosity, as proxies for economic activity and population distribution\n",
    "    - **Search Engine Trends**\n",
    "    - **Environmental, Social, and Governance (ESG) Data**\n",
    "\n",
    "\n",
    "<img src=\"images/text.png\" alt=\"Drawing\" width=\"400\"/>\n",
    "<img src=\"images/satt2.jpg\" alt=\"Drawing\" width=\"400\"/>\n",
    "<img src=\"images/satt.jpg\" alt=\"Drawing\" width=\"400\"/>\n",
    "<img src=\"images/satt3.jpg\" alt=\"Drawing\" width=\"400\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
